<!doctype>
<html prefix="og: http://ogp.me/ns#">
<head>
  <meta
		name="viewport"
		content="width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=no"
	>
	<meta http-equiv=Content-Type content="text/html; charset=UTF-8">
	<meta http-equiv="content-language" content="fr-fr">
  <meta name="robots" content="noindex,nofollow">
  <link href="./stylesheets/index.css" rel="stylesheet">
</head>
<body class="⭐">
  <header class="⭐-group header">
    <h1 class="⭐-h ⭐-h1">Title</h1>
  </header>
  <main>
    <section class="⭐-group ⭐-as-standalone" data-⭐-theme="lighter">
      <div>
        <h2 class="⭐-h ⭐-h2">H2 title</h2>
        <p class="⭐-p">
          Tolga Bolukbasi and colleagues recently posted an article about bias in what is learned with word2vec, on the standard Google News crawl (h/t Jack Clark). Essentially what they found is that word embeddings reflect stereotypes regarding gender (for instance, "nurse" is closer to "she" than "he" and "hero" is the reverse) and race ("black male" is closest to "assaulted" and "white male" to "entitled"). This is not hugely surprising, and it's nice to see it confirmed. The authors additionally present a method for removing those stereotypes with no cost (as measured with analogy tasks) to accuracy of the embeddings. This also shows up on twitter embeddings related to hate speech.
        </p>
      </div>
      <section class="⭐-group" data-⭐-theme="lighter">
        <h2 class="⭐-h ⭐-h2">H2 title</h2>
        <section class="⭐-group" data-⭐-theme="lighter">
          <h2 class="⭐-h ⭐-h2">H2 title</h2>
          <p class="⭐-p">
            Tolga Bolukbasi and colleagues recently posted an article about bias in what is learned with word2vec, on the standard Google News crawl (h/t Jack Clark). Essentially what they found is that word embeddings reflect stereotypes regarding gender (for instance, "nurse" is closer to "she" than "he" and "hero" is the reverse) and race ("black male" is closest to "assaulted" and "white male" to "entitled"). This is not hugely surprising, and it's nice to see it confirmed. The authors additionally present a method for removing those stereotypes with no cost (as measured with analogy tasks) to accuracy of the embeddings. This also shows up on twitter embeddings related to hate speech.
          </p>
        </section>
      </section>
    </section>
  </main>
  <h6 class="⭐-h ⭐-h6 ⭐-as-default">Small title</h6>
  <h5 class="⭐-h ⭐-h5 ⭐-as-default">Minor title</h5>
  <h4 class="⭐-h ⭐-h4 ⭐-as-default">Subtitle</h4>
  <h3 class="⭐-h ⭐-h3 ⭐-as-default">Title</h3>
  <h2 class="⭐-h ⭐-h2 ⭐-as-default">Large title</h2>
  <h1 class="⭐-h ⭐-h1 ⭐-as-default">Biggest title</h1>
  <p class="⭐-p">
    Tolga Bolukbasi and colleagues recently posted an article about bias in what is learned with word2vec, on the standard Google News crawl (h/t Jack Clark). Essentially what they found is that word embeddings reflect stereotypes regarding gender (for instance, "nurse" is closer to "she" than "he" and "hero" is the reverse) and race ("black male" is closest to "assaulted" and "white male" to "entitled"). This is not hugely surprising, and it's nice to see it confirmed. The authors additionally present a method for removing those stereotypes with no cost (as measured with analogy tasks) to accuracy of the embeddings. This also shows up on twitter embeddings related to hate speech.
  </p>
  <p class="⭐-p">
    There have been a handful of reactions to this work, some questioning the core motivation, essentially variants of "if there are biases in the data, they're there for a reason, and removing them is removing important information." The authors give a nice example in the paper (web search; two identical web pages about CS; one mentions "John" and the other "Mary"; query for "computer science" ranks the "John" one higher because of embeddings; appeal to a not-universally-held-belief that this is bad).
  </p>
  <footer>
    <small>A footer</small>
  </footer>
</body>
