<!doctype>
<html prefix="og: http://ogp.me/ns#">
<head>
  <meta
		name="viewport"
		content="width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=no"
	>
	<meta http-equiv=Content-Type content="text/html; charset=UTF-8">
	<meta http-equiv="content-language" content="fr-fr">
  <meta name="robots" content="noindex,nofollow">
  <link href="./stylesheets/index.css" rel="stylesheet">
</head>
<body class="⭐" data-⭐-theme="lighter">
  <header class="⭐-header ⭐-as-standalone">
    <h1 class="⭐-h ⭐-h1">Largest heading in header</h1>
    <pre class="⭐-pre">
      Tolga Bolukbasi and colleagues recently posted an article about bias in what is learned with word2vec, on the standard Google News crawl (h/t Jack Clark). Essentially what they found is that word embeddings reflect stereotypes reg.
    </pre>
  </header>
  <main class="⭐-group--root">
    <chapter class="⭐-chapter ⭐-group ⭐-as-standalone">
      <h1 class="⭐-h ⭐-h1 ⭐-as-default">Biggest title</h1>
      <h2 class="⭐-h ⭐-h2 ⭐-as-default">Large title</h2>
      <h3 class="⭐-h ⭐-h3 ⭐-as-default">Title</h3>
      <h4 class="⭐-h ⭐-h4 ⭐-as-default">Subtitle</h4>
      <h5 class="⭐-h ⭐-h5 ⭐-as-default">Minor title</h5>
      <h6 class="⭐-h ⭐-h6 ⭐-as-default">Small title</h6>
      <p class="⭐-p ⭐-as-body">
        Tolga Bolukbasi and colleagues recently posted an article about bias in what is learned with word2vec, on the standard Google News crawl (h/t Jack Clark). Essentially what they found is that word embeddings reflect stereotypes regarding gender (for instance, "nurse" is closer to "she" than "he" and "hero" is the reverse) and race ("black male" is closest to "assaulted" and "white male" to "entitled"). This is not hugely surprising, and it's nice to see it confirmed. The authors additionally present a method for removing those stereotypes with no cost (as measured with analogy tasks) to accuracy of the embeddings. This also shows up on twitter embeddings related to hate speech.
      </p>
      <p class="⭐-p ⭐-as-body ⭐-is-justified">
        Tolga Bolukbasi and colleagues recently posted an article about bias in what is learned with word2vec, on the standard Google News crawl (h/t Jack Clark). Essentially what they found is that word embeddings reflect stereotypes regarding gender (for instance, "nurse" is closer to "she" than "he" and "hero" is the reverse) and race ("black male" is closest to "assaulted" and "white male" to "entitled"). This is not hugely surprising, and it's nice to see it confirmed. The authors additionally present a method for removing those stereotypes with no cost (as measured with analogy tasks) to accuracy of the embeddings. This also shows up on twitter embeddings related to hate speech.
      </p>
      <p class="⭐-p">
        There have been a handful of reactions to this work, some questioning the core motivation, essentially variants of "if there are biases in the data, they're there for a reason, and removing them is removing important information." The authors give a nice example in the paper (web search; two identical web pages about CS; one mentions "John" and the other "Mary"; query for "computer science" ranks the "John" one higher because of embeddings; appeal to a not-universally-held-belief that this is bad).
      </p>
    </chapter>
    <chapter class="⭐-chapter ⭐-as-columns">
      <p class="⭐-p ⭐-as-body ⭐-is-justified">
        Tolga Bolukbasi and colleagues recently posted an article about bias in what is learned with word2vec, on the standard Google News crawl (h/t Jack Clark). Essentially what they found is that word embeddings reflect stereotypes regarding gender (for instance, "nurse" is closer to "she" than "he" and "hero" is the reverse) and race ("black male" is closest to "assaulted" and "white male" to "entitled"). This is not hugely surprising, and it's nice to see it confirmed. The authors additionally present a method for removing those stereotypes with no cost (as measured with analogy tasks) to accuracy of the embeddings. This also shows up on twitter embeddings related to hate speech.
      </p>
      <p class="⭐-p ⭐-as-body ⭐-is-justified">
        Tolga Bolukbasi and colleagues recently posted an article about bias in what is learned with word2vec, on the standard Google News crawl (h/t Jack Clark). Essentially what they found is that word embeddings reflect stereotypes regarding gender (for instance, "nurse" is closer to "she" than "he" and "hero" is the reverse) and race ("black male" is closest to "assaulted" and "white male" to "entitled"). This is not hugely surprising, and it's nice to see it confirmed. The authors additionally present a method for removing those stereotypes with no cost (as measured with analogy tasks) to accuracy of the embeddings. This also shows up on twitter embeddings related to hate speech.
      </p>
      <p class="⭐-p ⭐-as-body ⭐-is-justified">
        Tolga Bolukbasi and colleagues recently posted an article about bias in what is learned with word2vec, on the standard Google News crawl (h/t Jack Clark). Essentially what they found is that word embeddings reflect stereotypes regarding gender (for instance, "nurse" is closer to "she" than "he" and "hero" is the reverse) and race ("black male" is closest to "assaulted" and "white male" to "entitled"). This is not hugely surprising, and it's nice to see it confirmed. The authors additionally present a method for removing those stereotypes with no cost (as measured with analogy tasks) to accuracy of the embeddings. This also shows up on twitter embeddings related to hate speech.
      </p>
    </chapter>
  </main>
  <footer class="⭐-footer">
    <small class="⭐-small">A footer</small>
  </footer>
</body>
